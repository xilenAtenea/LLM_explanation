{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM: Question-answering with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model is designed to meet the queries of students, teachers and collaborators of the Universidad Autónoma de Occidente. It uses the Ollama platform, with the purpose of taking advantage of pre-trained open source models, such as Llama3, in a local way, together with the Langchain framework. This framework facilitates the implementation of the application of the extended language model to answer questions, in this case, using RAG (Retrieval Augmented Generation). It is important to note that this application is an initial prototype, intended to explore the use of large language models with RAG with a limited amount of data to operate locally. However, the long-term goal is to migrate it to a server with enhanced capabilities, which will allow training the model with a larger amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/rag.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager \n",
    "from langchain.llms import Ollama\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the variables of the model and embedding we are going to use, in this case the model is Llama3, an open source model released by Meta with 8 billion parameters and the embedding is a large context length text encoder called nomic-embed-text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL = \"llama3\" # Meta's open source model.\n",
    "EMBEDDING   = \"nomic-embed-text\" # A high-performing open embedding model with a large token context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the list of object loaders representing different document sources: three local PDF files and a web URL. These sources are loaded thanks to Langchain's \"PyPDFLoader\" and \"WebBaseLoader\" loaders. It then iterates over each of these loaders and loads the corresponding documents using the load() method. Finally, all loaded documents are grouped in a list called docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    PyPDFLoader(\"info/Código-de-Ética-para-Estudiantes.pdf\"),\n",
    "    PyPDFLoader(\"info/resolucion_No666.pdf\" ),\n",
    "    PyPDFLoader(\"info/Resolucion_de_Rectoria_No._7714_reglamento_academico_de_pregrado_Modalidad_Virtual.pdf\" ),\n",
    "    WebBaseLoader(\"https://www.uao.edu.co/solicitudes-de-supletorios-validaciones-y-habilitaciones/\")\n",
    "    ]\n",
    "\n",
    "docs = []\n",
    "\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# print (docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/split.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the object \"text_splitter\" which uses the RecursiveCharacterTextSplitter method of langchain where we define a fragment size of 1500 characters and an overlap of 150 characters. Then, it uses this splitter to split a list of documents (docs) into smaller fragments that will comply with the parameters defined in the splitter, and will be stored in the variable \"all_splits\". Subsequently, it prints the first two generated splits and displays the total amount of splits obtained to analyze the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter( \n",
    "    chunk_size=1500, chunk_overlap=150)\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# print(all_splits[0])\n",
    "# print(all_splits[1])\n",
    "# len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/chunks.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/storage.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code establishes a persistence directory, called \"persist_directory\", which stores the vector store used by the model to search for relevant documents that can be helpful to answer user queries. The vector store is created using Chroma, a lightweight, in-memory implementation. It is given as input the previously generated fragments (all_splits) and the embedding model defined in the variables, which is associated with Ollama. It is important to note that the persistence directory where the vector store will be stored for future use is also given as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'data' # This is where the vector store will be for future use.\n",
    "\n",
    "vectorstore = Chroma.from_documents( # Chroma is a Vector Store.  Chroma is lightweight and in memory making it easy to start with.\n",
    "    documents=all_splits, # Splits created earlier.\n",
    "    embedding=OllamaEmbeddings(model=EMBEDDING), #Embedding model.\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does a vector store work?\n",
    "\n",
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/create_vectorstore.png\" alt=\"Error al cargar la imagen\">\n",
    "    <img src=\"imgs/n_most_similar.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector store is a database in which we can easily search for similar vectors later on.  This is useful when trying to find relevant documents relates to a question. So, when we want to get an answer for a question, we create embeddings of the question and then the embeddings of the question are compared with all the different vectors in the vector store and choose the n most similar ones. Finally, we take the n most similar chunks, pass them along with the question to an LLM and get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(base_url=\"http://localhost:11434\",\n",
    "                                  model=LOCAL_MODEL,\n",
    "                                  verbose=True, # Provide additional information about what the program is doing\n",
    "                                  stop = ['<|eot_id|>'], # Prevent an infinite loop.\n",
    "                                  callback_manager=CallbackManager(\n",
    "                                      [StreamingStdOutCallbackHandler()]) # Monitor the model output while it is running\n",
    "                                  )\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this moment, the model for question-answering has been created, which uses Llama3 integrated through Ollama. A stop parameter has been configured to prevent the model from entering a loop once the user has made his query, ensuring that it only provides the requested answer. In addition, a callback has been implemented to monitor the output of the model while it is running and the verbose option to obtain a detailed result. The retriever has also been generated, which is built from the previously created vector store. This retriever will be used later to retrieve the relevant documents stored in the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the code begins by defining a prompt template, which acts as a schema for structuring the inputs to be passed to the chatbot model. This template includes sections for the context of the conversation, the history of previous interactions, the current question and the chatbot's response. This template is then used in the defined prompt in which the input variables are also made explicit. Then, a conversation memory buffer is configured to temporarily store the chatbot's conversation history. This buffer is responsible for retaining the dialog flow, allowing the chatbot to remember previous interactions in order to generate more coherent and contextual responses. In the next code block you can see some more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The prompt template has instructions about how to use the context. It also has placeholders for variables like context, \n",
    "history, question, and the answer.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\" You are a chatbot providing information to students, collaborators, \n",
    "and professors of Universidad Autónoma de Occidente. Your tone should be professional \n",
    "and informative. Keep responses concise, addressing only what users ask. If you don't \n",
    "know the answer, simply state so. You have to answer in the language used by the user. \n",
    "Use the following context to response the questions.\n",
    "    \n",
    "    Context: {context}\n",
    "    History: {history}\n",
    "\n",
    "    Question: {question}\n",
    "    Chatbot:\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "A prompt is the style of creating inputs to pass into the model.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        input_variables=[\"history\", \"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Buffer (region of a memory used to store data temporarily) for storing conversation memory\n",
    "\"\"\"\n",
    "memory = ConversationBufferMemory(\n",
    "        memory_key=\"history\", # Specifies the key to be used to store the conversation memory.\n",
    "        return_messages=True, # Set that the messages stored in the memory will be returned.\n",
    "        input_key=\"question\" # Specifies the key that will be used to store the conversation entries in memory.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the retrieval process using RetrievalQA. This RetrievalQA uses the language model (LLM) previously established in the code, along with the information retriever configured using the vectore store. Arguments such as the prompt, the memory for the model's question and answer history, and the verbose option for detailed output are provided. At this stage, the relevant documents and the original question are processed through the LLM, prompting the model to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/retrieval.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LLM chain is the combination of an LLM and a Prompt.\n",
    "RetrievalQA does question answering backed by a retrieval step. This is created\n",
    "by passing a language model and vector database as a retriever.\n",
    "\"\"\"\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=retriever,\n",
    "            verbose=True,\n",
    "            chain_type_kwargs={\n",
    "                \"verbose\": True,\n",
    "                \"prompt\": prompt,\n",
    "                \"memory\": memory\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the \"stuff\" method as a chain_type, which puts all documents in the final prompt. This involves a single call to the language model. But if we have too many documents, they may not fit in the context window. In such a case, we can use different techniques such as map-reduce, refine and map_rerank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does the RetrievalQA work?\n",
    "\n",
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/RetrievalQA.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the picture, the user's question is sent to the vector store. Here, it's compared with the documents inside to find the most relevant ones – those that are similar to the user's question. These relevant documents, along with the user's original question, are then given to the LLM model. Using this information, the LLM generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This will allow us to display LLM responses in a user-friendly format.\n",
    "\"\"\"\n",
    "\n",
    "def pretty_print(answer):\n",
    "    print(f\"\"\"\n",
    "      UAO CHATBOT\n",
    "            \n",
    "      Usurario: {answer.get(\"query\")} \n",
    "      Chat: {answer.get(\"result\")}\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <!-- Este HTML centra la imagen -->\n",
    "    <img src=\"imgs/output.png\" alt=\"Error al cargar la imagen\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests are conducted with several questions in Spanish (as it is the main language used at the target university), one of which involves direct interaction with the user. In addition, a question is presented in English to illustrate the model's ability to respond in the language in which the user is addressed. It should be noted that a final test was conducted in Japanese, in which the model responded correctly, albeit in English, demonstrating its limitations in dealing with different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"¿Cuál es la misión de la universidad autónoma de occidente?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      UAO CHATBOT\n",
      "            \n",
      "      Usurario: ¿Cuál es la misión de la universidad autónoma de occidente? \n",
      "      Chat: Amigo estudiante, colaborador o profesor de la Universidad Autónoma del Occidente (UAO), te brindo información acerca de la misión de esta institución educativa.\n",
      "\n",
      "La misión de la UAO es \"Ser una institución líder en el desarrollo integral del ser humano, a través de la formación académica, investigadora y comunitaria, enfocada en la calidad, innovación y sostenibilidad\".\n",
      "\n",
      "Espero que esta información te sea útil. ¿Tienes alguna otra pregunta o necesitas más detalles?\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "pretty_print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Para la resolución de problemas ¿cual es el conducto regular?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_chain.invoke({\"query\": query2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      UAO CHATBOT\n",
      "            \n",
      "      Usurario: Para la resolución de problemas ¿cual es el conducto regular? \n",
      "      Chat: Amigo estudiante, colaborador o profesor de la Universidad Autónoma del Occidente (UAO), te brindo información acerca del proceso regular para la resolución de problemas en esta institución educativa.\n",
      "\n",
      "En la UAO, el conducto regular para la resolución de problemas es el siguiente:\n",
      "\n",
      "1. El estudiante presenta su problema o inquietud a través de diferentes canales, como el correo electrónico [email protected], la plataforma virtual del sitio web de la universidad o personalmente en las oficinas de atención al estudiante.\n",
      "2. La oficina de atención al estudiante o el área correspondiente analiza y evalúa la solicitud para determinar la mejor forma de abordar el problema.\n",
      "3. Si es necesario, se puede convocar a una reunión con un representante de la universidad para discutir la situación y encontrar una solución adecuada.\n",
      "4. La universidad hace todo lo posible por resolver el problema de manera eficiente y justa, considerando las circunstancias específicas del caso.\n",
      "\n",
      "Espero que esta información te sea útil. ¿Tienes alguna otra pregunta o necesitas más detalles?\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "pretty_print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"Resume tu anterior respuesta en 10 palabras.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa_chain.invoke({\"query\": query3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      UAO CHATBOT\n",
      "            \n",
      "      Usurario: Resume tu anterior respuesta en 10 palabras. \n",
      "      Chat: La misión de la UAO es formar seres humanos integrales y sostenibles.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "pretty_print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "User interaction\n",
    "\"\"\"\n",
    "\n",
    "query = str(input(\"\"\"\n",
    "                  BIENVENID@ A CHAT UAO \n",
    "                  ¿En qué te puedo ayudar?\n",
    "                  -> \n",
    "                  \"\"\"))\n",
    "\n",
    "answer = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      UAO CHATBOT\n",
      "            \n",
      "      Usurario: ¿Cómo solicito un supletorio? \n",
      "      Chat: Excelente pregunta! En la Universidad Autónoma del Occidente (UAO), si necesitas solicitar un supletorio, puedes hacerlo a través de los siguientes canales:\n",
      "\n",
      "1. Correo electrónico: Puedes enviar un correo electrónico a [email protected] con el número de identificación (NIT) o código de estudiante y describir la situación que te llevó a solicitar el supletorio.\n",
      "2. Plataforma virtual del sitio web de la universidad: Puedes acceder a la plataforma virtual y buscar la sección correspondiente para solicitar un supletorio.\n",
      "3. Oficinas de atención al estudiante: Puedes visitar las oficinas de atención al estudiante en persona o por teléfono para solicitar el supletorio.\n",
      "\n",
      "Es importante tener en cuenta que es necesario presentar justificativos y documentación adicional según sea necesario, por lo que asegúrate de estar preparado antes de solicitar el supletorio.\n",
      "\n",
      "¿Tienes alguna otra pregunta o necesitas más detalles?\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "pretty_print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_query = \"What is the mission of the University Autonoma de Occidente\"\n",
    "english_answer = qa_chain.invoke({\"query\": english_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      UAO CHATBOT\n",
      "            \n",
      "      Usurario: What is the mission of the University Autonoma de Occidente \n",
      "      Chat: According to the provided information, Universidad Autónoma de Occidente (UAO) is a university that provides education and research opportunities. The university's personería jurídica was granted by the Gobernación del Valle del Cauca on February 20th, 1970, and its institutional accreditation of high quality was granted by the Ministerio de Educación Nacional on November 13th, 2003.\n",
      "\n",
      "UAO is a private institution that is recognized and regulated by the Ministry of Education. Its mission is to provide high-quality education, promote research and innovation, and contribute to the development of society through the training of professionals and the generation of knowledge.\n",
      "\n",
      "The university has several campuses, including Campus Valle del Lili, which is located in Jamundí. UAO also has a virtual campus that offers online programs and courses.\n",
      "\n",
      "In terms of academic performance, UAO has established guidelines for students who do not meet the minimum standards for their program. This includes the possibility of applying for a prueba académica (academic test) if a student fails to pass two subjects in a cycle or loses three times the same subject.\n",
      "\n",
      "If you have any specific questions about Universidad Autónoma de Occidente or its academic programs, I'll be happy to help!\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "pretty_print(english_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_query = \"オクシデンテ自治大学の使命とは？\" # What is the mission of the University Autonoma de Occidente?\n",
    "japanese_answer = qa_chain.invoke({\"query\": japanese_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      UAO CHATBOT\n",
      "            \n",
      "      Usurario: オクシデンテ自治大学の使命とは？ \n",
      "      Chat: According to the provided information, Universidad Autónoma de Occidente (UAO) is a university that provides education and research opportunities. The university's personería jurídica was granted by the Gobernación del Valle del Cauca on February 20th, 1970, and its institutional accreditation of high quality was granted by the Ministerio de Educación Nacional on November 13th, 2003.\n",
      "\n",
      "UAO is a private institution that is recognized and regulated by the Ministry of Education. Its mission is to provide high-quality education, promote research and innovation, and contribute to the development of society through the training of professionals and the generation of knowledge.\n",
      "\n",
      "In summary, the mission of Universidad Autónoma de Occidente (UAO) is to provide high-quality education, promote research and innovation, and contribute to the development of society through the training of professionals and the generation of knowledge.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "pretty_print(japanese_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images reference\n",
    "Using langchain for Question Answering on Own Data - Onkar Mishra - Medium\n",
    "By Onkar Mishra Container: Medium Publisher: Medium Year: 2023 URL: \n",
    "https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
